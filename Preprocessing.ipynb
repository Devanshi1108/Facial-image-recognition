{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "data_dirs = ['/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/FER2013', '/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/AffectNetHQ', '/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/Tiny']\n",
    "\n",
    "for dir_path in data_dirs:\n",
    "    images = os.listdir(dir_path)\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    for num_images in [5000, 10000, 15000]:\n",
    "        if num_images > len(images):\n",
    "            print(f\"Directory '{dir_path}' does not have enough images ({len(images)}) for {num_images} samples.\")\n",
    "            continue\n",
    "        \n",
    "        output_dir = os.path.join(dir_path, f\"{num_images}_samples\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            image_path = os.path.join(dir_path, images[i])\n",
    "            output_path = os.path.join(output_dir, images[i])\n",
    "            shutil.copy(image_path, output_path)\n",
    "            \n",
    "        print(f\"{num_images} samples created in directory '{output_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dirs = ['/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/FER2013', '/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/AffectNetHQ', '/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/Tiny']\n",
    "\n",
    "val_split = 0.16\n",
    "test_split = 0.2\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        train_images, test_val_images = train_test_split(os.listdir(class_dir), test_size=test_split+val_split)\n",
    "        val_images, test_images = train_test_split(test_val_images, test_size=test_split/(test_split+val_split))\n",
    "\n",
    "        for image_name in train_images:\n",
    "            src_path = os.path.join(class_dir, image_name)\n",
    "            dst_path = os.path.join(train_dir, class_name, image_name)\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "        for image_name in val_images:\n",
    "            src_path = os.path.join(class_dir, image_name)\n",
    "            dst_path = os.path.join(val_dir, class_name, image_name)\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "        for image_name in test_images:\n",
    "            src_path = os.path.join(class_dir, image_name)\n",
    "            dst_path = os.path.join(test_dir, class_name, image_name)\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            shutil.copy(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "dir_path = \"/Users/jatin/Desktop/Winter'23/AI/Project/Datasets\"\n",
    "\n",
    "for subdir, _, files in os.walk(dir_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "            img_path = os.path.join(subdir, file)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "            img_resized = cv2.resize(img, (224, 224))\n",
    "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "            cv2.imwrite(img_path, img_gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def batch_mean_and_sd(loader):   \n",
    "    \n",
    "    cnt = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "\n",
    "    for images, _ in loader:\n",
    "        b, c, h, w = images.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(images, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(images ** 2,\n",
    "                                  dim=[0, 2, 3])\n",
    "        fst_moment = (cnt * fst_moment + sum_) / (\n",
    "                      cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_square) / (\n",
    "                            cnt + nb_pixels)\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    mean, std = fst_moment, torch.sqrt(\n",
    "      snd_moment - fst_moment ** 2)        \n",
    "    return mean,std\n",
    "  \n",
    "data_dirs = ['/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/FER2013', '/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/AffectNetHQ', '/Users/jatin/Desktop/Winter\\'23/AI/Project/Datasets/Tiny']\n",
    "\n",
    "for dir_path in data_dirs:\n",
    "    data = datasets.ImageFolder(root=dir_path, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(data, batch_size=32, shuffle=True)\n",
    "    mean, std = batch_mean_and_sd(train_loader)\n",
    "    print(f\"Directory: {dir_path}, Mean: {mean}, Standard Deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = datasets.ImageFolder('/kaggle/input/affectnethq/AffectNetHQ/train/', transform=transform)\n",
    "dataset2 = datasets.ImageFolder('/kaggle/input/fer2013/FER2013/train/', transform=transform)\n",
    "dataset3 = datasets.ImageFolder('/kaggle/input/tinydataset/Tiny/train', transform=transform)\n",
    "\n",
    "# Define dataloaders\n",
    "dataloader1 = torch.utils.data.DataLoader(dataset1, batch_size=32, shuffle=True)\n",
    "dataloader2 = torch.utils.data.DataLoader(dataset2, batch_size=32, shuffle=True)\n",
    "dataloader3 = torch.utils.data.DataLoader(dataset3, batch_size=32, shuffle=True)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean1, std1 = {}, {}\n",
    "mean2, std2 = {}, {}\n",
    "mean3, std3 = {}, {}\n",
    "n1, n2, n3 = {}, {}, {}\n",
    "\n",
    "for data, target in dataloader1:\n",
    "    for i in range(data.size(0)):\n",
    "        class_name = dataset1.classes[target[i]]\n",
    "        if class_name not in mean1:\n",
    "            mean1[class_name], std1[class_name] = 0., 0.\n",
    "            n1[class_name] = 0\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean1[class_name] += data[i].mean()\n",
    "        std1[class_name] += data[i].std()\n",
    "        n1[class_name] += 1\n",
    "\n",
    "for data, target in dataloader2:\n",
    "    for i in range(data.size(0)):\n",
    "        class_name = dataset2.classes[target[i]]\n",
    "        if class_name not in mean2:\n",
    "            mean2[class_name], std2[class_name] = 0., 0.\n",
    "            n2[class_name] = 0\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean2[class_name] += data[i].mean()\n",
    "        std2[class_name] += data[i].std()\n",
    "        n2[class_name] += 1\n",
    "\n",
    "for data, target in dataloader3:\n",
    "    for i in range(data.size(0)):\n",
    "        class_name = dataset3.classes[target[i]]\n",
    "        if class_name not in mean3:\n",
    "            mean3[class_name], std3[class_name] = 0., 0.\n",
    "            n3[class_name] = 0\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean3[class_name] += data[i].mean()\n",
    "        std3[class_name] += data[i].std()\n",
    "        n3[class_name] += 1\n",
    "\n",
    "for class_name in mean1:\n",
    "    mean1[class_name] /= n1[class_name]\n",
    "    std1[class_name] /= n1[class_name]\n",
    "\n",
    "for class_name in mean2:\n",
    "    mean2[class_name] /= n2[class_name]\n",
    "    std2[class_name] /= n2[class_name]\n",
    "\n",
    "for class_name in mean3:\n",
    "    mean3[class_name] /= n3[class_name]\n",
    "    std3[class_name] /= n3[class_name]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].bar(mean1.keys(), mean1.values(), label='AffectNetHQ',zorder=3)\n",
    "axs[0].bar(mean2.keys(), mean2.values(), label='FER2013')\n",
    "axs[0].bar(mean3.keys(), mean3.values(), label='Tiny', zorder=2)\n",
    "axs[0].set_title('Mean')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].bar(std1.keys(), std1.values(), label='AffectNetHQ')\n",
    "axs[1].bar(std2.keys(), std2.values(), label='FER2013')\n",
    "axs[1].bar(std3.keys(), std3.values(), label='Tiny', zorder=2)\n",
    "axs[1].set_title('Standard Deviation')\n",
    "axs[1].legend()\n",
    "\n",
    "labels = list(set(list(mean1.keys()) + list(mean2.keys()) + list(mean3.keys())))\n",
    "axs[0].set_xticks(range(len(labels)))\n",
    "axs[0].set_xticklabels(labels, rotation=90)\n",
    "axs[1].set_xticks(range(len(labels)))\n",
    "axs[1].set_xticklabels(labels, rotation=90)\n",
    "axs[0].set_ylim([0.425, 0.6])\n",
    "axs[1].set_ylim([0.19, 0.25])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
